<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="KinDER: A Physical Reasoning Benchmark for Robot Learning and Planning">
    <title>KinDER: A Physical Reasoning Benchmark for Robot Learning and Planning</title>
    <link rel="icon" type="image/svg+xml" href="assets/favicon.svg">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
</head>
<body>
{{HEADER}}

    <main>
        <section class="title-authors-section">
            <div class="container">
                <h1>KinDER: A Physical Reasoning Benchmark<br class="title-break"> for Robot Learning and Planning</h1>

                <p class="submission-info">Anonymous Submission. Under Review @ RSS 2026</p>
            </div>
        </section>

{{HERO_SECTION}}

        <section id="about">
            <div class="container">
                <h2>About KinDER</h2>
                <p class="intro-text">A physical reasoning benchmark for robot learning and planning.</p>

                <div class="about-content">
                    <p>Robotic systems that interact with the physical world must reason about kinematic and dynamic constraints imposed by their own embodiment, their environment, and the task at hand. We introduce KinDER, a benchmark for Kinematic and Dynamic Embodied Reasoning that targets physical reasoning challenges arising in robot learning and planning. KinDER comprises 25 procedurally generated environments, a Gymnasium-compatible Python library with parameterized skills and demonstrations, and a standardized evaluation suite with 8 implemented baselines spanning task and motion planning, imitation learning, reinforcement learning, and foundation-model-based approaches. The environments are designed to isolate five core physical reasoning challenges: basic spatial relations, nonprehensile multi-object manipulation, tool use, combinatorial geometric constraints, and dynamic constraints, disentangled from perception, language understanding, and application-specific complexity. Empirical evaluation shows that existing methods struggle to solve many of the environments, indicating substantial gaps in current approaches to physical reasoning. We additionally include real-to-sim-to-real experiments on a mobile manipulator to assess the correspondence between simulation and real-world physical interaction. KinDER is fully open-sourced and intended to enable systematic comparison across diverse paradigms for advancing physical reasoning in robotics.</p>

                </div>

                <h3 class="collapsible-header" onclick="toggleCollapsible('challenges-section')">
                    What Makes KinDER Challenging? <span class="toggle-icon" id="challenges-toggle">▼</span>
                </h3>

                <div class="collapsible-content" id="challenges-section">
                    <div class="challenges-grid">
                        <div class="challenge-card">
                            <h4>For Reinforcement Learning</h4>
                            <p>Environments have long horizons and sparse rewards. Users are welcome to engineer dense rewards, but doing so may be nontrivial. Environments also have very diverse task distributions, so learned policies must generalize.</p>
                        </div>

                        <div class="challenge-card">
                            <h4>For Imitation Learning</h4>
                            <p>Physical reasoning requires understanding physical constraints (e.g., spatial relationships, kinematics, dynamics). Imitating surface-level patterns in demonstrations is not enough to generalize to broad task distributions.</p>
                        </div>

                        <div class="challenge-card">
                            <h4>For Vision-Language Models</h4>
                            <p>The physical reasoning required in KinDER is not easy to represent in natural language. Spatial reasonining, a subset of physical reasoning, is a known challenge for vision-language models.</p>
                        </div>

                        <div class="challenge-card">
                            <h4>For Hierarchical Approaches</h4>
                            <p>Approaches that first decide "what to do" and then decide "how to do it" -- whether in hierarchical RL, planning, or with multi-level foundation models -- will run into difficulties when there are couplings between these high-level and low-level decisions.</p>
                        </div>

                        <div class="challenge-card">
                            <h4>For Task and Motion Planning</h4>
                            <p>KinDER does not provide any models for TAMP. Users are welcome to engineer their own, but doing so may be nontrivial. Some environments contain many objects, which may make planning slow.</p>
                        </div>

                        <div class="challenge-card">
                            <h4>For Human Engineers</h4>
                            <p>KinDER features diverse task distributions and long time horizons, making it challenging for engineers to hand-design even very environment-specific solutions. A single task may be straightforward, but designing generalized solutions is nontrivial.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="usage">
            <div class="container">
                <h2>Usage</h2>
                <h3>Installation</h3>

                <p>Coming soon.</p>

                <!-- <p>KinDER will be available on PyPI when it is ready. At that time, you will be able to install it with <code>pip install kinder</code>. Until then, you can install it from source as follows. First, we strongly recommend <a href="https://docs.astral.sh/uv/getting-started/installation/" target="_blank">uv</a>. Then choose one of the following based on your needs:</p>

                <div class="code-block">
                    <pre><code class="language-bash"># Core dependencies only
uv pip install -r optional_prpl_requirements/core.txt && uv pip install -e .

# Everything (excluding develop)
uv pip install -e ".[all]"

# Specific environments
uv pip install -e ".[geom2d]"      # Geometric 2D only
uv pip install -e ".[dynamic2d]"   # Dynamic 2D only
uv pip install -e ".[geom3d]"      # Geometric 3D only
uv pip install -e ".[tidybot]"     # TidyBot only

# Compositional
uv pip install -e ".[geom2d,geom3d]"</code></pre>
                </div>

                <h3>Basic Usage (Gym API)</h3>
                <div class="code-block">
                    <pre><code class="language-python">import kinder
kinder.register_all_environments()
env = kinder.make("kinder/Obstruction2D-o3-v0")  # 3 obstructions
obs, info = env.reset()  # procedural generation
action = env.action_space.sample()
next_obs, reward, terminated, truncated, info = env.step(action)
img = env.render()</code></pre>
                </div> -->

                <h3 class="collapsible-header" onclick="toggleCollapsible('object-centric-section')">
                    Object-Centric States <span class="toggle-icon" id="object-centric-toggle">▼</span>
                </h3>

                <div class="collapsible-content" id="object-centric-section">
                    <p>All environments in KinDER use object-centric states:</p>

                    <div class="code-block">
                        <pre><code class="language-python">from kinder.envs.geom2d.obstruction2d import ObjectCentricObstruction2DEnv
env = ObjectCentricObstruction2DEnv(num_obstructions=3)
obs, _ = env.reset(seed=123)
print(obs.pretty_str())</code></pre>
                    </div>

                    <p>Here, <code>obs</code> is an <a href="https://github.com/tomsilver/relational-structs/blob/main/src/relational_structs/object_centric_state.py#L25" target="_blank">ObjectCentricState</a>, and the printout is:</p>

                    <div class="code-block">
                        <pre><code>############################################################### STATE ###############################################################
type: crv_robot           x         y    theta    base_radius    arm_joint    arm_length    vacuum    gripper_height    gripper_width
-----------------  --------  --------  -------  -------------  -----------  ------------  --------  ----------------  ---------------
robot              0.885039  0.803795  -1.5708            0.1          0.1           0.2         0              0.07             0.01

type: rectangle           x         y    theta    static    color_r    color_g    color_b    z_order      width     height
-----------------  --------  --------  -------  --------  ---------  ---------  ---------  ---------  ---------  ---------
obstruction0       0.422462  0.100001        0         0       0.75        0.1        0.1        100  0.132224   0.0766399
obstruction1       0.804663  0.100001        0         0       0.75        0.1        0.1        100  0.0805652  0.0955062
obstruction2       0.559246  0.100001        0         0       0.75        0.1        0.1        100  0.12608    0.180172

type: target_block          x         y    theta    static    color_r    color_g    color_b    z_order     width    height
--------------------  -------  --------  -------  --------  ---------  ---------  ---------  ---------  --------  --------
target_block          1.20082  0.100001        0         0   0.501961          0   0.501961        100  0.138302  0.155183

type: target_surface           x    y    theta    static    color_r    color_g    color_b    z_order     width    height
----------------------  --------  ---  -------  --------  ---------  ---------  ---------  ---------  --------  --------
target_surface          0.499675    0        0         1   0.501961          0   0.501961        101  0.180286       0.1
#####################################################################################################################################</code></pre>
                    </div>

                    <p>For compatibility with baselines, observations are provided as vectors. Convert between vectors and object-centric states:</p>

                    <div class="code-block">
                        <pre><code class="language-python">import kinder
kinder.register_all_environments()
env = kinder.make("kinder/Obstruction2D-o3-v0")
vec_obs, _ = env.reset(seed=123)
object_centric_obs = env.observation_space.devectorize(vec_obs)
recovered_vec_obs = env.observation_space.vectorize(object_centric_obs)</code></pre>
                    </div>
                </div>
            </div>
        </section>

{{BENCHMARK_SECTION}}

{{RESULTS_SECTION}}

        <section id="real-robots">
            <div class="container">
                <h2>Real Robot Validation</h2>
                <div class="real-robots-content">
                    <img src="assets/real-sim-example.gif" alt="Real robot demonstration" class="real-robot-demo">
                    <img src="assets/real-sim-example-2.gif" alt="Real robot demonstration 2" class="real-robot-demo">
                    <p>Real robot versions of KinDER tasks with real-to-sim-to-real planning.</p>
                </div>
            </div>
        </section>

    </main>

{{FOOTER}}

    <script>
        function toggleCollapsible(id) {
            const content = document.getElementById(id);
            const icon = document.getElementById(id.replace('-section', '-toggle'));

            if (content.style.maxHeight) {
                content.style.maxHeight = null;
                icon.textContent = '▼';
            } else {
                content.style.maxHeight = content.scrollHeight + 'px';
                icon.textContent = '▲';
            }
        }

        // GIF hover play/pause functionality
        document.addEventListener('DOMContentLoaded', function() {
            const gifItems = document.querySelectorAll('.demo-gif-item');

            gifItems.forEach(item => {
                const img = item.querySelector('img');
                const gifSrc = item.getAttribute('data-gif');
                const staticSrc = item.getAttribute('data-static');

                // Start with static image (already set in HTML)
                item.classList.add('gif-paused');

                let loadTimeout;

                item.addEventListener('mouseenter', function() {
                    // Clear any pending timeout
                    clearTimeout(loadTimeout);

                    // Load and play the GIF with a cache-busting parameter to force restart
                    img.src = gifSrc + '?t=' + Date.now();
                    item.classList.remove('gif-paused');
                });

                item.addEventListener('mouseleave', function() {
                    // Switch back to static image after a short delay
                    loadTimeout = setTimeout(() => {
                        img.src = staticSrc;
                        item.classList.add('gif-paused');
                    }, 200);
                });
            });
        });
    </script>
</body>
</html>
